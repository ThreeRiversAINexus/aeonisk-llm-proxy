# LLM Batching Proxy Configuration
# Copy this file to .env and add your API keys

# ── API Keys (at least one required) ──

# OpenAI API Key (for GPT models and batch API)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models and batch API)
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# xAI API Key (for Grok models, direct only)
# Get from: https://console.x.ai/
XAI_API_KEY=your_xai_api_key_here

# Google Gemini API Key (direct only)
# Get from: https://aistudio.google.com/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# DeepInfra API Key (OpenAI-compatible, direct only)
# Get from: https://deepinfra.com/dash/api_keys
DEEPINFRA_API_KEY=your_deepinfra_api_key_here

# ── Batch Queue Settings ──

# Number of requests to accumulate before flushing to batch API
# BATCH_THRESHOLD=100

# Max seconds to wait before flushing queue (even if threshold not reached)
# BATCH_TIMEOUT=300

# Flush queue if no new requests arrive for this many seconds
# BATCH_MAX_IDLE=3600

# How often (seconds) to poll batch API for completion
# BATCH_POLL_INTERVAL=60
