# LLM Batching Proxy Configuration
# Copy this file to .env and add your API keys

# ── API Keys (at least one required) ──

# OpenAI API Key (for GPT models and batch API)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models and batch API)
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ── Batch Queue Settings ──

# Number of requests to accumulate before flushing to batch API
# BATCH_THRESHOLD=100

# Max seconds to wait before flushing queue (even if threshold not reached)
# BATCH_TIMEOUT=300

# Flush queue if no new requests arrive for this many seconds
# BATCH_MAX_IDLE=3600

# How often (seconds) to poll batch API for completion
# BATCH_POLL_INTERVAL=60
